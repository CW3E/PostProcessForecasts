{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/will/anaconda2/envs/post_process/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/will/anaconda2/envs/post_process/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/will/anaconda2/envs/post_process/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/will/anaconda2/envs/post_process/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/will/anaconda2/envs/post_process/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/will/anaconda2/envs/post_process/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import time\n",
    "import xarray as xr\n",
    "from netCDF4 import Dataset\n",
    "\n",
    "import keras\n",
    "from keras.layers import Input, Dense, merge, Embedding, Flatten, Concatenate,Conv2D,BatchNormalization,Dropout,MaxPooling2D\n",
    "from keras.models import Model, Sequential\n",
    "from keras.optimizers import Adam, SGD\n",
    "import keras.backend as K\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from cartopy import config\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cf\n",
    "from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER\n",
    "import palettable\n",
    "\n",
    "import random \n",
    "random.seed(1) #for reproduceability. \n",
    "import matplotlib.ticker as mticker\n",
    "from sklearn import datasets, linear_model, metrics \n",
    "# import utils\n",
    "\n",
    "if keras.backend.backend() == 'tensorflow':\n",
    "    from tensorflow import erf\n",
    "else:\n",
    "    from theano.tensor import erf\n",
    "# import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def crps_cost_function(y_true, y_pred, theano=False):\n",
    "    \"\"\"Compute the CRPS cost function for a normal distribution defined by\n",
    "    the mean and standard deviation.\n",
    "    Code inspired by Kai Polsterer (HITS).\n",
    "    Args:\n",
    "        y_true: True values\n",
    "        y_pred: Tensor containing predictions: [mean, std]\n",
    "        theano: Set to true if using this with pure theano.\n",
    "    Returns:\n",
    "        mean_crps: Scalar with mean CRPS over batch\n",
    "    \"\"\"\n",
    "\n",
    "    # Split input\n",
    "    mu = y_pred[:, 0]\n",
    "    sigma = y_pred[:, 1]\n",
    "    # Ugly workaround for different tensor allocation in keras and theano\n",
    "    if not theano:\n",
    "        y_true = y_true[:, 0]   # Need to also get rid of axis 1 to match!\n",
    "\n",
    "    # To stop sigma from becoming negative we first have to \n",
    "    # convert it the the variance and then take the square\n",
    "    # root again. \n",
    "    var = K.square(sigma)\n",
    "    # The following three variables are just for convenience\n",
    "    loc = (y_true - mu) / K.sqrt(var)\n",
    "    phi = 1.0 / np.sqrt(2.0 * np.pi) * K.exp(-K.square(loc) / 2.0)\n",
    "    Phi = 0.5 * (1.0 + erf(loc / np.sqrt(2.0)))\n",
    "    # First we will compute the crps for each input/target pair\n",
    "    crps =  K.sqrt(var) * (loc * (2. * Phi - 1.) + 2 * phi - 1. / np.sqrt(np.pi))\n",
    "    # Then we take the mean. The cost is now a scalar\n",
    "    return K.mean(crps)\n",
    "\n",
    "\n",
    "\n",
    "def build_EMOS_network_keras(compile=False, optimizer='sgd', lr=0.1):\n",
    "    \"\"\"Build (and maybe compile) EMOS network in keras.\n",
    "    Args:\n",
    "        compile: If true, compile model\n",
    "        optimizer: String of keras optimizer\n",
    "        lr: learning rate\n",
    "    Returns:\n",
    "        model: Keras model\n",
    "    \"\"\"\n",
    "    mean_in = Input(shape=(1,))\n",
    "    std_in = Input(shape=(1,))\n",
    "    mean_out = Dense(1, activation='linear')(mean_in)\n",
    "    std_out = Dense(1, activation='linear')(std_in)\n",
    "    x = keras.layers.concatenate([mean_out, std_out], axis=1)\n",
    "    model = Model(inputs=[mean_in, std_in], outputs=x)\n",
    "\n",
    "    if compile:\n",
    "        opt = keras.optimizers.__dict__[optimizer](lr=lr)\n",
    "        model.compile(optimizer=opt, loss=crps_cost_function)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def build_NN_network_keras(compile=False, optimizer='sgd', lr=0.1):\n",
    "    \"\"\"Build (and maybe compile) EMOS network in keras.\n",
    "    Args:\n",
    "        compile: If true, compile model\n",
    "        optimizer: String of keras optimizer\n",
    "        lr: learning rate\n",
    "    Returns:\n",
    "        model: Keras model\n",
    "    \"\"\"\n",
    "    mean_in = Input(shape=(1,))\n",
    "    std_in = Input(shape=(1,))\n",
    "    mean_out = Dense(1, activation='relu')(mean_in)\n",
    "    std_out = Dense(1, activation='relu')(std_in)\n",
    "    x = keras.layers.concatenate([mean_out, std_out], axis=1)\n",
    "    \n",
    "    D1 = Dense(10, activation='relu')(x)\n",
    "    D2 = Dense(2, activation='linear')(D1)\n",
    "    \n",
    "    model = Model(inputs=[mean_in, std_in], outputs=D2)\n",
    "\n",
    "    if compile:\n",
    "        opt = keras.optimizers.__dict__[optimizer](lr=lr)\n",
    "        model.compile(optimizer=opt, loss=crps_cost_function)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "AllDat = xr.open_zarr('/Users/will/Desktop/Haupt/PNA/CMAzar/')\n",
    "AllDat=AllDat.to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Analysis</th>\n",
       "      <th>Center</th>\n",
       "      <th>Ense</th>\n",
       "      <th>Forecast_Date</th>\n",
       "      <th>Forecast_Lead</th>\n",
       "      <th>PNA</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.148891</td>\n",
       "      <td>CMA</td>\n",
       "      <td>0</td>\n",
       "      <td>2007051512</td>\n",
       "      <td>0</td>\n",
       "      <td>0.148173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.634738</td>\n",
       "      <td>CMA</td>\n",
       "      <td>0</td>\n",
       "      <td>2007051512</td>\n",
       "      <td>1</td>\n",
       "      <td>0.592949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.300384</td>\n",
       "      <td>CMA</td>\n",
       "      <td>0</td>\n",
       "      <td>2007051512</td>\n",
       "      <td>2</td>\n",
       "      <td>0.291869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.246399</td>\n",
       "      <td>CMA</td>\n",
       "      <td>0</td>\n",
       "      <td>2007051512</td>\n",
       "      <td>3</td>\n",
       "      <td>0.204389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.420670</td>\n",
       "      <td>CMA</td>\n",
       "      <td>0</td>\n",
       "      <td>2007051512</td>\n",
       "      <td>4</td>\n",
       "      <td>0.381550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>CMA</td>\n",
       "      <td>14</td>\n",
       "      <td>2020013112</td>\n",
       "      <td>11</td>\n",
       "      <td>-1.853610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>CMA</td>\n",
       "      <td>14</td>\n",
       "      <td>2020013112</td>\n",
       "      <td>12</td>\n",
       "      <td>-1.542850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>CMA</td>\n",
       "      <td>14</td>\n",
       "      <td>2020013112</td>\n",
       "      <td>13</td>\n",
       "      <td>-1.526760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>CMA</td>\n",
       "      <td>14</td>\n",
       "      <td>2020013112</td>\n",
       "      <td>14</td>\n",
       "      <td>-1.630980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>CMA</td>\n",
       "      <td>14</td>\n",
       "      <td>2020013112</td>\n",
       "      <td>15</td>\n",
       "      <td>-1.569420</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>736832 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Analysis Center  Ense  Forecast_Date  Forecast_Lead       PNA\n",
       "index                                                               \n",
       "0      0.148891    CMA     0     2007051512              0  0.148173\n",
       "1      0.634738    CMA     0     2007051512              1  0.592949\n",
       "2      0.300384    CMA     0     2007051512              2  0.291869\n",
       "3      0.246399    CMA     0     2007051512              3  0.204389\n",
       "4      0.420670    CMA     0     2007051512              4  0.381550\n",
       "...         ...    ...   ...            ...            ...       ...\n",
       "235    0.000000    CMA    14     2020013112             11 -1.853610\n",
       "236    0.000000    CMA    14     2020013112             12 -1.542850\n",
       "237    0.000000    CMA    14     2020013112             13 -1.526760\n",
       "238    0.000000    CMA    14     2020013112             14 -1.630980\n",
       "239    0.000000    CMA    14     2020013112             15 -1.569420\n",
       "\n",
       "[736832 rows x 6 columns]"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AllDat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/will/anaconda2/envs/post_process/lib/python3.6/site-packages/ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/Users/will/anaconda2/envs/post_process/lib/python3.6/site-packages/ipykernel_launcher.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/Users/will/anaconda2/envs/post_process/lib/python3.6/site-packages/ipykernel_launcher.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "leadtime=7 #days\n",
    "monthz = 2 #month to post-process with EMOS \n",
    "AllDatLead = AllDat.loc[AllDat['Forecast_Lead']==leadtime]\n",
    "#create a day,month,year column:\n",
    "\n",
    "datetime_str = '09/19/18 13:55:26'\n",
    "datetime_object = datetime.strptime(datetime_str, '%m/%d/%y %H:%M:%S')\n",
    "\n",
    "dayz = []\n",
    "mons = []\n",
    "yrs = []\n",
    "\n",
    "for bb,dat in enumerate((AllDatLead['Forecast_Date'])):\n",
    "    datetime_object = datetime.strptime(str(dat), '%Y%m%d%H')\n",
    "    yrs.append(datetime_object.year)\n",
    "    mons.append(datetime_object.month)\n",
    "    dayz.append(datetime_object.day)\n",
    "    \n",
    "AllDatLead['day']=dayz\n",
    "AllDatLead['month']=mons\n",
    "AllDatLead['year']=yrs\n",
    "\n",
    "# #select the month we'd like to post-process\n",
    "# AllDatMon = AllDatLead.loc[AllDatLead['month'] == monthz]\n",
    "AllDatMon = AllDatLead\n",
    "\n",
    "\n",
    "PNA_mean=[]\n",
    "PNA_std=[]\n",
    "PNA_obs=[]\n",
    "For_Date=[]\n",
    "\n",
    "#create mean and standard deviation for the Ensemble forecast: \n",
    "for nn,dat in enumerate(np.unique(AllDatMon['Forecast_Date'])):\n",
    "    tt = AllDatMon[AllDatMon.Forecast_Date==dat].PNA\n",
    "    PNA_mean.append(np.mean(AllDatMon[AllDatMon.Forecast_Date==dat].PNA))\n",
    "    PNA_std.append(np.std(AllDatMon[AllDatMon.Forecast_Date==dat].PNA))\n",
    "    PNA_obs.append(np.mean(AllDatMon[AllDatMon.Forecast_Date==dat].Analysis))\n",
    "    For_Date.append(dat)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create mean and standard deviation for the Ensemble forecast: \n",
    "PNA_mean=[]\n",
    "PNA_std=[]\n",
    "PNA_obs=[]\n",
    "For_Date=[]\n",
    "yz=[]\n",
    "mz=[]\n",
    "dz=[]\n",
    "\n",
    "for nn,dat in enumerate(np.unique(AllDatMon['Forecast_Date'])):\n",
    "    tt = AllDatMon[AllDatMon.Forecast_Date==dat].PNA\n",
    "    PNA_mean.append(np.mean(AllDatMon[AllDatMon.Forecast_Date==dat].PNA))\n",
    "    PNA_std.append(np.std(AllDatMon[AllDatMon.Forecast_Date==dat].PNA))\n",
    "    PNA_obs.append(np.mean(AllDatMon[AllDatMon.Forecast_Date==dat].Analysis))\n",
    "    dz.append(np.mean(AllDatMon[AllDatMon.Forecast_Date==dat].day))\n",
    "    mz.append(np.mean(AllDatMon[AllDatMon.Forecast_Date==dat].month))\n",
    "    yz.append(np.mean(AllDatMon[AllDatMon.Forecast_Date==dat].year))\n",
    "    For_Date.append(dat)\n",
    "\n",
    "    \n",
    "d = {'PNA_mean': PNA_mean, 'PNA_std': PNA_std,'PNA_obs':PNA_obs,'Fore_Date':For_Date,\n",
    "     'day':dz,'month':mz,'year':yz}\n",
    "PNAdf  = pd.DataFrame(data=d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "PNA_train = PNAdf[PNAdf.year<=2017]\n",
    "PNA_validate = PNAdf[(PNAdf.year>=2018)&(PNAdf.year<2019)]\n",
    "PNA_test = PNAdf[(PNAdf.year>=2019)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split: Train,Test,Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "xm=np.array(PNA_train['PNA_mean'])\n",
    "xs=np.array(PNA_train['PNA_std'])\n",
    "y=np.array(PNA_train['PNA_obs'])\n",
    "\n",
    "xm_v=np.array(PNA_validate['PNA_mean'])\n",
    "xs_v=np.array(PNA_validate['PNA_std'])\n",
    "y_v=np.array(PNA_validate['PNA_obs'])\n",
    "\n",
    "xm_t=np.array(PNA_test['PNA_mean'])\n",
    "xs_t=np.array(PNA_test['PNA_std'])\n",
    "y_t=np.array(PNA_test['PNA_obs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_31\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_61 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_62 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_95 (Dense)                (None, 1)            2           input_61[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_96 (Dense)                (None, 1)            2           input_62[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_31 (Concatenate)    (None, 2)            0           dense_95[0][0]                   \n",
      "                                                                 dense_96[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 4\n",
      "Trainable params: 4\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "emos = build_EMOS_network_keras(compile=True, optimizer='sgd', lr=0.1)\n",
    "emos.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2993 samples, validate on 342 samples\n",
      "Epoch 1/100\n",
      "2993/2993 [==============================] - 1s 315us/step - loss: 0.2460 - val_loss: 0.2334\n",
      "Epoch 2/100\n",
      "2993/2993 [==============================] - 0s 49us/step - loss: 0.2435 - val_loss: 0.2330\n",
      "Epoch 3/100\n",
      "2993/2993 [==============================] - 0s 52us/step - loss: 0.2433 - val_loss: 0.2299\n",
      "Epoch 4/100\n",
      "2993/2993 [==============================] - 0s 49us/step - loss: 0.2433 - val_loss: 0.2311\n",
      "Epoch 5/100\n",
      "2993/2993 [==============================] - 0s 49us/step - loss: 0.2427 - val_loss: 0.2316\n",
      "Epoch 6/100\n",
      "2993/2993 [==============================] - 0s 49us/step - loss: 0.2424 - val_loss: 0.2288\n",
      "Epoch 7/100\n",
      "2993/2993 [==============================] - 0s 49us/step - loss: 0.2426 - val_loss: 0.2315\n",
      "Epoch 8/100\n",
      "2993/2993 [==============================] - 0s 50us/step - loss: 0.2420 - val_loss: 0.2286\n",
      "Epoch 9/100\n",
      "2993/2993 [==============================] - 0s 50us/step - loss: 0.2422 - val_loss: 0.2309\n",
      "Epoch 10/100\n",
      "2993/2993 [==============================] - 0s 49us/step - loss: 0.2421 - val_loss: 0.2322\n",
      "Epoch 11/100\n",
      "2993/2993 [==============================] - 0s 49us/step - loss: 0.2421 - val_loss: 0.2324\n",
      "Epoch 12/100\n",
      "2993/2993 [==============================] - 0s 49us/step - loss: 0.2419 - val_loss: 0.2295\n",
      "Epoch 13/100\n",
      "2993/2993 [==============================] - 0s 49us/step - loss: 0.2418 - val_loss: 0.2333\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.010000000149011612.\n",
      "Epoch 14/100\n",
      "2993/2993 [==============================] - 0s 51us/step - loss: 0.2416 - val_loss: 0.2321\n",
      "Epoch 15/100\n",
      "2993/2993 [==============================] - 0s 50us/step - loss: 0.2414 - val_loss: 0.2312\n",
      "Epoch 16/100\n",
      "2993/2993 [==============================] - 0s 50us/step - loss: 0.2413 - val_loss: 0.2308\n",
      "Epoch 17/100\n",
      "2993/2993 [==============================] - 0s 50us/step - loss: 0.2413 - val_loss: 0.2305\n",
      "Epoch 18/100\n",
      "2993/2993 [==============================] - 0s 50us/step - loss: 0.2413 - val_loss: 0.2303\n",
      "Restoring model weights from the end of the best epoch\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 00018: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1b23ed3898>"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn=20\n",
    "epcs=100\n",
    "#### KERAS CALLBACKS TO ADD to Training######\n",
    "filp = '/where/your/best/model/is/saved'\n",
    "svbst = keras.callbacks.callbacks.ModelCheckpoint(filp, monitor='val_loss', \n",
    "                                                  verbose=1, save_best_only=True, save_weights_only=False)\n",
    "#add this to the callbacks in fit function to save the best model on your personal machine. \n",
    "\n",
    "earlystop = keras.callbacks.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=10, \n",
    "                                                    verbose=1, mode='auto', restore_best_weights=True) \n",
    "rdclr = keras.callbacks.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, verbose=1, \n",
    "                                                    mode='auto', min_delta=0.0001, cooldown=0, min_lr=0)\n",
    "\n",
    "#### Fitting the Model ######\n",
    "emos.fit([xm,xs],y,batch_size=bn,validation_data=[[xm_v,xs_v],y_v],epochs=epcs,callbacks=[earlystop,rdclr])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a prediction:\n",
    "preds = emos.predict([xm_t, xs_t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Forecast_Time</th>\n",
       "      <th>Obs</th>\n",
       "      <th>Emos_mean</th>\n",
       "      <th>Emos_std</th>\n",
       "      <th>Model_mean</th>\n",
       "      <th>Model_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3335</th>\n",
       "      <td>2019010112</td>\n",
       "      <td>-0.102586</td>\n",
       "      <td>-0.102080</td>\n",
       "      <td>0.340791</td>\n",
       "      <td>-0.257838</td>\n",
       "      <td>0.236883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3336</th>\n",
       "      <td>2019010212</td>\n",
       "      <td>0.308736</td>\n",
       "      <td>0.151038</td>\n",
       "      <td>0.425267</td>\n",
       "      <td>0.052651</td>\n",
       "      <td>0.328747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3337</th>\n",
       "      <td>2019010312</td>\n",
       "      <td>0.198227</td>\n",
       "      <td>-0.161174</td>\n",
       "      <td>0.344373</td>\n",
       "      <td>-0.330326</td>\n",
       "      <td>0.240778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3338</th>\n",
       "      <td>2019010412</td>\n",
       "      <td>0.830801</td>\n",
       "      <td>0.490254</td>\n",
       "      <td>0.299558</td>\n",
       "      <td>0.468752</td>\n",
       "      <td>0.192043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3339</th>\n",
       "      <td>2019010512</td>\n",
       "      <td>0.519359</td>\n",
       "      <td>0.544322</td>\n",
       "      <td>0.306831</td>\n",
       "      <td>0.535075</td>\n",
       "      <td>0.199952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3696</th>\n",
       "      <td>2020012712</td>\n",
       "      <td>-0.039734</td>\n",
       "      <td>-0.153388</td>\n",
       "      <td>0.386627</td>\n",
       "      <td>-0.320775</td>\n",
       "      <td>0.286727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3697</th>\n",
       "      <td>2020012812</td>\n",
       "      <td>-0.020386</td>\n",
       "      <td>0.271076</td>\n",
       "      <td>0.381503</td>\n",
       "      <td>0.199896</td>\n",
       "      <td>0.281155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3698</th>\n",
       "      <td>2020012912</td>\n",
       "      <td>-0.035568</td>\n",
       "      <td>0.113824</td>\n",
       "      <td>0.360397</td>\n",
       "      <td>0.007002</td>\n",
       "      <td>0.258203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3699</th>\n",
       "      <td>2020013012</td>\n",
       "      <td>-0.036261</td>\n",
       "      <td>0.027995</td>\n",
       "      <td>0.436108</td>\n",
       "      <td>-0.098281</td>\n",
       "      <td>0.340536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3700</th>\n",
       "      <td>2020013112</td>\n",
       "      <td>-0.046873</td>\n",
       "      <td>-0.265093</td>\n",
       "      <td>0.453784</td>\n",
       "      <td>-0.457799</td>\n",
       "      <td>0.359758</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>366 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Forecast_Time       Obs  Emos_mean  Emos_std  Model_mean  Model_std\n",
       "3335     2019010112 -0.102586  -0.102080  0.340791   -0.257838   0.236883\n",
       "3336     2019010212  0.308736   0.151038  0.425267    0.052651   0.328747\n",
       "3337     2019010312  0.198227  -0.161174  0.344373   -0.330326   0.240778\n",
       "3338     2019010412  0.830801   0.490254  0.299558    0.468752   0.192043\n",
       "3339     2019010512  0.519359   0.544322  0.306831    0.535075   0.199952\n",
       "...             ...       ...        ...       ...         ...        ...\n",
       "3696     2020012712 -0.039734  -0.153388  0.386627   -0.320775   0.286727\n",
       "3697     2020012812 -0.020386   0.271076  0.381503    0.199896   0.281155\n",
       "3698     2020012912 -0.035568   0.113824  0.360397    0.007002   0.258203\n",
       "3699     2020013012 -0.036261   0.027995  0.436108   -0.098281   0.340536\n",
       "3700     2020013112 -0.046873  -0.265093  0.453784   -0.457799   0.359758\n",
       "\n",
       "[366 rows x 6 columns]"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = {'Forecast_Time': PNA_test.Fore_Date,'Obs': PNA_test.PNA_obs,'Emos_mean': preds[:,0],'Emos_std': preds[:,1],\n",
    "    'Model_mean':PNA_test.PNA_mean,'Model_std':PNA_test.PNA_std}\n",
    "results_df = pd.DataFrame(d)\n",
    "#Sorting DataFrame by time and Station ID\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "366/366 [==============================] - 0s 23us/step\n"
     ]
    }
   ],
   "source": [
    "crps_preds = emos.evaluate([xm_t,xs_t],y_t)\n",
    "#jump through hoops to get data in the right form for loss function:\n",
    "MODpna_pred = np.transpose(np.array([results_df.Model_mean,results_df.Model_std]))\n",
    "crps_mod= keras.backend.eval(crps_cost_function(np.expand_dims(y_t,axis=1),MODpna_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post-Processed with EMOS = CRPS: 0.21587287402543864\n",
      "Raw Ensemble a Global = CRPS: 0.23820415125847338\n"
     ]
    }
   ],
   "source": [
    "print('Post-Processed with EMOS = CRPS:',crps_preds)\n",
    "print('Raw Ensemble a Global = CRPS:',crps_mod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try with a Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_32\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_63 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_64 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_97 (Dense)                (None, 1)            2           input_63[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_98 (Dense)                (None, 1)            2           input_64[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_32 (Concatenate)    (None, 2)            0           dense_97[0][0]                   \n",
      "                                                                 dense_98[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_99 (Dense)                (None, 10)           30          concatenate_32[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_100 (Dense)               (None, 2)            22          dense_99[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 56\n",
      "Trainable params: 56\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "nn = build_NN_network_keras(compile=True, optimizer='sgd', lr=0.1)\n",
    "nn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2993 samples, validate on 342 samples\n",
      "Epoch 1/100\n",
      "2993/2993 [==============================] - 1s 349us/step - loss: 0.2714 - val_loss: 0.2620\n",
      "Epoch 2/100\n",
      "2993/2993 [==============================] - 0s 55us/step - loss: 0.2491 - val_loss: 0.2490\n",
      "Epoch 3/100\n",
      "2993/2993 [==============================] - 0s 56us/step - loss: 0.2445 - val_loss: 0.2521\n",
      "Epoch 4/100\n",
      "2993/2993 [==============================] - 0s 54us/step - loss: 0.2430 - val_loss: 0.2408\n",
      "Epoch 5/100\n",
      "2993/2993 [==============================] - 0s 56us/step - loss: 0.2433 - val_loss: 0.2378\n",
      "Epoch 6/100\n",
      "2993/2993 [==============================] - 0s 54us/step - loss: 0.2427 - val_loss: 0.2385\n",
      "Epoch 7/100\n",
      "2993/2993 [==============================] - 0s 54us/step - loss: 0.2426 - val_loss: 0.2365\n",
      "Epoch 8/100\n",
      "2993/2993 [==============================] - 0s 53us/step - loss: 0.2419 - val_loss: 0.2386\n",
      "Epoch 9/100\n",
      "2993/2993 [==============================] - 0s 53us/step - loss: 0.2427 - val_loss: 0.2376\n",
      "Epoch 10/100\n",
      "2993/2993 [==============================] - 0s 53us/step - loss: 0.2423 - val_loss: 0.2369\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.010000000149011612.\n",
      "Epoch 11/100\n",
      "2993/2993 [==============================] - 0s 53us/step - loss: 0.2404 - val_loss: 0.2358\n",
      "Epoch 12/100\n",
      "2993/2993 [==============================] - 0s 54us/step - loss: 0.2403 - val_loss: 0.2354\n",
      "Epoch 13/100\n",
      "2993/2993 [==============================] - 0s 54us/step - loss: 0.2404 - val_loss: 0.2359\n",
      "Epoch 14/100\n",
      "2993/2993 [==============================] - 0s 53us/step - loss: 0.2404 - val_loss: 0.2354\n",
      "Epoch 15/100\n",
      "2993/2993 [==============================] - 0s 53us/step - loss: 0.2402 - val_loss: 0.2366\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 16/100\n",
      "2993/2993 [==============================] - 0s 53us/step - loss: 0.2404 - val_loss: 0.2358\n",
      "Epoch 17/100\n",
      "2993/2993 [==============================] - 0s 54us/step - loss: 0.2402 - val_loss: 0.2356\n",
      "Epoch 18/100\n",
      "2993/2993 [==============================] - 0s 52us/step - loss: 0.2401 - val_loss: 0.2354\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 19/100\n",
      "2993/2993 [==============================] - 0s 61us/step - loss: 0.2401 - val_loss: 0.2354\n",
      "Epoch 20/100\n",
      "2993/2993 [==============================] - 0s 60us/step - loss: 0.2401 - val_loss: 0.2354\n",
      "Epoch 21/100\n",
      "2993/2993 [==============================] - 0s 56us/step - loss: 0.2401 - val_loss: 0.2354\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 22/100\n",
      "2993/2993 [==============================] - 0s 61us/step - loss: 0.2401 - val_loss: 0.2354\n",
      "Epoch 23/100\n",
      "2993/2993 [==============================] - 0s 59us/step - loss: 0.2401 - val_loss: 0.2354\n",
      "Epoch 24/100\n",
      "2993/2993 [==============================] - 0s 58us/step - loss: 0.2401 - val_loss: 0.2354\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 25/100\n",
      "2993/2993 [==============================] - 0s 54us/step - loss: 0.2401 - val_loss: 0.2354\n",
      "Epoch 26/100\n",
      "2993/2993 [==============================] - 0s 54us/step - loss: 0.2401 - val_loss: 0.2354\n",
      "Epoch 27/100\n",
      "2993/2993 [==============================] - 0s 55us/step - loss: 0.2401 - val_loss: 0.2354\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-08.\n",
      "Epoch 28/100\n",
      "2993/2993 [==============================] - 0s 57us/step - loss: 0.2401 - val_loss: 0.2354\n",
      "Epoch 29/100\n",
      "2993/2993 [==============================] - 0s 62us/step - loss: 0.2401 - val_loss: 0.2354\n",
      "Epoch 30/100\n",
      "2993/2993 [==============================] - 0s 59us/step - loss: 0.2401 - val_loss: 0.2354\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 9.999998695775504e-09.\n",
      "Epoch 31/100\n",
      "2993/2993 [==============================] - 0s 55us/step - loss: 0.2401 - val_loss: 0.2354\n",
      "Epoch 32/100\n",
      "2993/2993 [==============================] - 0s 53us/step - loss: 0.2401 - val_loss: 0.2354\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00032: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1b70e3f9e8>"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn=20\n",
    "epcs=100\n",
    "#### KERAS CALLBACKS TO ADD to Training######\n",
    "filp = '/where/your/best/model/is/saved'\n",
    "svbst = keras.callbacks.callbacks.ModelCheckpoint(filp, monitor='val_loss', \n",
    "                                                  verbose=1, save_best_only=True, save_weights_only=False)\n",
    "#add this to the callbacks in fit function to save the best model on your personal machine. \n",
    "\n",
    "earlystop = keras.callbacks.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=20, \n",
    "                                                    verbose=1, mode='auto', restore_best_weights=True) \n",
    "rdclr = keras.callbacks.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1, \n",
    "                                                    mode='auto', min_delta=0.0001, cooldown=0, min_lr=0)\n",
    "\n",
    "#### Fitting the Model ######\n",
    "nn.fit([xm,xs],y,batch_size=bn,validation_data=[[xm_v,xs_v],y_v],epochs=epcs,callbacks=[earlystop,rdclr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Forecast_Time</th>\n",
       "      <th>Obs</th>\n",
       "      <th>Emos_mean</th>\n",
       "      <th>Emos_std</th>\n",
       "      <th>Model_mean</th>\n",
       "      <th>Model_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3335</th>\n",
       "      <td>2019010112</td>\n",
       "      <td>-0.102586</td>\n",
       "      <td>-0.122834</td>\n",
       "      <td>-0.373830</td>\n",
       "      <td>-0.257838</td>\n",
       "      <td>0.236883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3336</th>\n",
       "      <td>2019010212</td>\n",
       "      <td>0.308736</td>\n",
       "      <td>0.135451</td>\n",
       "      <td>-0.419044</td>\n",
       "      <td>0.052651</td>\n",
       "      <td>0.328747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3337</th>\n",
       "      <td>2019010312</td>\n",
       "      <td>0.198227</td>\n",
       "      <td>-0.179630</td>\n",
       "      <td>-0.367424</td>\n",
       "      <td>-0.330326</td>\n",
       "      <td>0.240778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3338</th>\n",
       "      <td>2019010412</td>\n",
       "      <td>0.830801</td>\n",
       "      <td>0.445675</td>\n",
       "      <td>-0.437097</td>\n",
       "      <td>0.468752</td>\n",
       "      <td>0.192043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3339</th>\n",
       "      <td>2019010512</td>\n",
       "      <td>0.519359</td>\n",
       "      <td>0.499227</td>\n",
       "      <td>-0.444837</td>\n",
       "      <td>0.535075</td>\n",
       "      <td>0.199952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3696</th>\n",
       "      <td>2020012712</td>\n",
       "      <td>-0.039734</td>\n",
       "      <td>-0.165725</td>\n",
       "      <td>-0.375875</td>\n",
       "      <td>-0.320775</td>\n",
       "      <td>0.286727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3697</th>\n",
       "      <td>2020012812</td>\n",
       "      <td>-0.020386</td>\n",
       "      <td>0.245338</td>\n",
       "      <td>-0.425561</td>\n",
       "      <td>0.199896</td>\n",
       "      <td>0.281155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3698</th>\n",
       "      <td>2020012912</td>\n",
       "      <td>-0.035568</td>\n",
       "      <td>0.089593</td>\n",
       "      <td>-0.403058</td>\n",
       "      <td>0.007002</td>\n",
       "      <td>0.258203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3699</th>\n",
       "      <td>2020013012</td>\n",
       "      <td>-0.036261</td>\n",
       "      <td>0.017699</td>\n",
       "      <td>-0.406307</td>\n",
       "      <td>-0.098281</td>\n",
       "      <td>0.340536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3700</th>\n",
       "      <td>2020013112</td>\n",
       "      <td>-0.046873</td>\n",
       "      <td>-0.262662</td>\n",
       "      <td>-0.419614</td>\n",
       "      <td>-0.457799</td>\n",
       "      <td>0.359758</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>366 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Forecast_Time       Obs  Emos_mean  Emos_std  Model_mean  Model_std\n",
       "3335     2019010112 -0.102586  -0.122834 -0.373830   -0.257838   0.236883\n",
       "3336     2019010212  0.308736   0.135451 -0.419044    0.052651   0.328747\n",
       "3337     2019010312  0.198227  -0.179630 -0.367424   -0.330326   0.240778\n",
       "3338     2019010412  0.830801   0.445675 -0.437097    0.468752   0.192043\n",
       "3339     2019010512  0.519359   0.499227 -0.444837    0.535075   0.199952\n",
       "...             ...       ...        ...       ...         ...        ...\n",
       "3696     2020012712 -0.039734  -0.165725 -0.375875   -0.320775   0.286727\n",
       "3697     2020012812 -0.020386   0.245338 -0.425561    0.199896   0.281155\n",
       "3698     2020012912 -0.035568   0.089593 -0.403058    0.007002   0.258203\n",
       "3699     2020013012 -0.036261   0.017699 -0.406307   -0.098281   0.340536\n",
       "3700     2020013112 -0.046873  -0.262662 -0.419614   -0.457799   0.359758\n",
       "\n",
       "[366 rows x 6 columns]"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#make a prediction:\n",
    "preds = nn.predict([xm_t, xs_t])\n",
    "\n",
    "d = {'Forecast_Time': PNA_test.Fore_Date,'Obs': PNA_test.PNA_obs,'Emos_mean': preds[:,0],'Emos_std': preds[:,1],\n",
    "    'Model_mean':PNA_test.PNA_mean,'Model_std':PNA_test.PNA_std}\n",
    "results_df = pd.DataFrame(d)\n",
    "#Sorting DataFrame by time and Station ID\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "366/366 [==============================] - 0s 25us/step\n"
     ]
    }
   ],
   "source": [
    "crps_preds = nn.evaluate([xm_t,xs_t],y_t)\n",
    "#jump through hoops to get data in the right form for loss function:\n",
    "MODpna_pred = np.transpose(np.array([results_df.Model_mean,results_df.Model_std]))\n",
    "crps_mod= keras.backend.eval(crps_cost_function(np.expand_dims(y_t,axis=1),MODpna_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post-Processed with EMOS = CRPS: 0.22985542292803363\n",
      "Raw Ensemble a Global = CRPS: 0.23820415125847338\n"
     ]
    }
   ],
   "source": [
    "print('Post-Processed with EMOS = CRPS:',crps_preds)\n",
    "print('Raw Ensemble a Global = CRPS:',crps_mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (post_process)",
   "language": "python",
   "name": "post_process"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
